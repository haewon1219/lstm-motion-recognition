{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking files in: C:/Users/Admin/Desktop/data\\back\\좌표\n",
      "Checking files in: C:/Users/Admin/Desktop/data\\squat\\좌표\n",
      "Checking files in: C:/Users/Admin/Desktop/data\\side\\좌표\n",
      "Data shape: (150, 90, 33, 3), Labels shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 데이터 경로 설정\n",
    "base_dir = \"C:/Users/Admin/Desktop/data\"\n",
    "categories = [\"back\", \"squat\", \"side\"]\n",
    "\n",
    "def load_data(base_dir, categories, seq_len=90):\n",
    "    \"\"\"\n",
    "    데이터 로드 및 크기 조정 (패딩 또는 잘라내기).\n",
    "    Args:\n",
    "        base_dir (str): 데이터가 저장된 기본 경로.\n",
    "        categories (list): 행동 카테고리 리스트.\n",
    "        seq_len (int): 고정된 시퀀스 길이.\n",
    "    Returns:\n",
    "        np.array: 데이터 배열.\n",
    "        np.array: 라벨 배열.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    label_map = {category: idx for idx, category in enumerate(categories)}  # 라벨 매핑\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(base_dir, category, \"좌표\")\n",
    "        print(f\"Checking files in: {category_path}\")\n",
    "        for i in range(1, 51):  # keypoints_1 ~ keypoints_50\n",
    "            file_path = os.path.join(category_path, f\"keypoints_{i}.npy\")\n",
    "            if os.path.exists(file_path):\n",
    "                keypoints = np.load(file_path)  # 데이터 로드\n",
    "                if keypoints.shape[0] > seq_len:  # 길이가 seq_len보다 길면 잘라내기\n",
    "                    keypoints = keypoints[:seq_len]\n",
    "                elif keypoints.shape[0] < seq_len:  # 길이가 seq_len보다 짧으면 패딩\n",
    "                    pad_width = seq_len - keypoints.shape[0]\n",
    "                    keypoints = np.pad(keypoints, ((0, pad_width), (0, 0), (0, 0)), mode='constant')\n",
    "                data.append(keypoints)\n",
    "                labels.append(label_map[category])\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# 수정된 데이터 로드 실행\n",
    "data, labels = load_data(base_dir, categories)\n",
    "print(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 경로 및 라벨 매핑\n",
    "data_paths = {\n",
    "    \"back\": \"C:/Users/Admin/Desktop/data/back/좌표\",\n",
    "    \"squat\": \"C:/Users/Admin/Desktop/data/squat/좌표\",\n",
    "    \"side\": \"C:/Users/Admin/Desktop/data/side/좌표\"\n",
    "}\n",
    "labels_map = {\"back\": 0, \"squat\": 1, \"side\": 2}  # 라벨 매핑\n",
    "\n",
    "def load_data(data_paths, labels_map, seq_len=90):\n",
    "    \"\"\"\n",
    "    데이터 로드 및 크기 조정 (패딩 또는 잘라내기).\n",
    "    Args:\n",
    "        data_paths (dict): 카테고리별 데이터 경로.\n",
    "        labels_map (dict): 카테고리와 라벨 매핑.\n",
    "        seq_len (int): 고정된 시퀀스 길이.\n",
    "    Returns:\n",
    "        np.array: 데이터 배열.\n",
    "        np.array: 라벨 배열.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    for category, path in data_paths.items():\n",
    "        print(f\"Checking files in: {path}\")\n",
    "        for i in range(1, 51):  # keypoints_1 ~ keypoints_50\n",
    "            file_path = os.path.join(path, f\"keypoints_{i}.npy\")\n",
    "            if os.path.exists(file_path):\n",
    "                keypoints = np.load(file_path)  # 데이터 로드\n",
    "                if keypoints.shape[0] > seq_len:  # 길이가 seq_len보다 길면 잘라내기\n",
    "                    keypoints = keypoints[:seq_len]\n",
    "                elif keypoints.shape[0] < seq_len:  # 길이가 seq_len보다 짧으면 패딩\n",
    "                    pad_width = seq_len - keypoints.shape[0]\n",
    "                    keypoints = np.pad(keypoints, ((0, pad_width), (0, 0), (0, 0)), mode='constant')\n",
    "                data.append(keypoints)\n",
    "                labels.append(labels_map[category])\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking files in: C:/Users/Admin/Desktop/data/back/좌표\n",
      "['keypoints_1.npy', 'keypoints_10.npy', 'keypoints_11.npy', 'keypoints_12.npy', 'keypoints_13.npy', 'keypoints_14.npy', 'keypoints_15.npy', 'keypoints_16.npy', 'keypoints_17.npy', 'keypoints_18.npy', 'keypoints_19.npy', 'keypoints_2.npy', 'keypoints_20.npy', 'keypoints_21.npy', 'keypoints_22.npy', 'keypoints_23.npy', 'keypoints_24.npy', 'keypoints_25.npy', 'keypoints_26.npy', 'keypoints_27.npy', 'keypoints_28.npy', 'keypoints_29.npy', 'keypoints_3.npy', 'keypoints_30.npy', 'keypoints_31.npy', 'keypoints_32.npy', 'keypoints_33.npy', 'keypoints_34.npy', 'keypoints_35.npy', 'keypoints_36.npy', 'keypoints_37.npy', 'keypoints_38.npy', 'keypoints_39.npy', 'keypoints_4.npy', 'keypoints_40.npy', 'keypoints_41.npy', 'keypoints_42.npy', 'keypoints_43.npy', 'keypoints_44.npy', 'keypoints_45.npy', 'keypoints_46.npy', 'keypoints_47.npy', 'keypoints_48.npy', 'keypoints_49.npy', 'keypoints_5.npy', 'keypoints_50.npy', 'keypoints_6.npy', 'keypoints_7.npy', 'keypoints_8.npy', 'keypoints_9.npy']\n",
      "Checking files in: C:/Users/Admin/Desktop/data/squat/좌표\n",
      "['keypoints_1.npy', 'keypoints_10.npy', 'keypoints_11.npy', 'keypoints_12.npy', 'keypoints_13.npy', 'keypoints_14.npy', 'keypoints_15.npy', 'keypoints_16.npy', 'keypoints_17.npy', 'keypoints_18.npy', 'keypoints_19.npy', 'keypoints_2.npy', 'keypoints_20.npy', 'keypoints_21.npy', 'keypoints_22.npy', 'keypoints_23.npy', 'keypoints_24.npy', 'keypoints_25.npy', 'keypoints_26.npy', 'keypoints_27.npy', 'keypoints_28.npy', 'keypoints_29.npy', 'keypoints_3.npy', 'keypoints_30.npy', 'keypoints_31.npy', 'keypoints_32.npy', 'keypoints_33.npy', 'keypoints_34.npy', 'keypoints_35.npy', 'keypoints_36.npy', 'keypoints_37.npy', 'keypoints_38.npy', 'keypoints_39.npy', 'keypoints_4.npy', 'keypoints_40.npy', 'keypoints_41.npy', 'keypoints_42.npy', 'keypoints_43.npy', 'keypoints_44.npy', 'keypoints_45.npy', 'keypoints_46.npy', 'keypoints_47.npy', 'keypoints_48.npy', 'keypoints_49.npy', 'keypoints_5.npy', 'keypoints_50.npy', 'keypoints_6.npy', 'keypoints_7.npy', 'keypoints_8.npy', 'keypoints_9.npy']\n",
      "Checking files in: C:/Users/Admin/Desktop/data/side/좌표\n",
      "['keypoints_1.npy', 'keypoints_10.npy', 'keypoints_11.npy', 'keypoints_12.npy', 'keypoints_13.npy', 'keypoints_14.npy', 'keypoints_15.npy', 'keypoints_16.npy', 'keypoints_17.npy', 'keypoints_18.npy', 'keypoints_19.npy', 'keypoints_2.npy', 'keypoints_20.npy', 'keypoints_21.npy', 'keypoints_22.npy', 'keypoints_23.npy', 'keypoints_24.npy', 'keypoints_25.npy', 'keypoints_26.npy', 'keypoints_27.npy', 'keypoints_28.npy', 'keypoints_29.npy', 'keypoints_3.npy', 'keypoints_30.npy', 'keypoints_31.npy', 'keypoints_32.npy', 'keypoints_33.npy', 'keypoints_34.npy', 'keypoints_35.npy', 'keypoints_36.npy', 'keypoints_37.npy', 'keypoints_38.npy', 'keypoints_39.npy', 'keypoints_4.npy', 'keypoints_40.npy', 'keypoints_41.npy', 'keypoints_42.npy', 'keypoints_43.npy', 'keypoints_44.npy', 'keypoints_45.npy', 'keypoints_46.npy', 'keypoints_47.npy', 'keypoints_48.npy', 'keypoints_49.npy', 'keypoints_5.npy', 'keypoints_50.npy', 'keypoints_6.npy', 'keypoints_7.npy', 'keypoints_8.npy', 'keypoints_9.npy']\n",
      "Checking files in: C:/Users/Admin/Desktop/data/back/좌표\n",
      "Checking files in: C:/Users/Admin/Desktop/data/squat/좌표\n",
      "Checking files in: C:/Users/Admin/Desktop/data/side/좌표\n",
      "데이터 크기: (150, 90, 33, 3) (150,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for label_name, path in data_paths.items():\n",
    "    print(f\"Checking files in: {path}\")\n",
    "    print(os.listdir(path))\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_data(data_paths, labels_map)\n",
    "\n",
    "# 데이터 크기 확인\n",
    "print(\"데이터 크기:\", X.shape, y.shape)  # Expected output: (30, frames, joints, coords), (30,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(data):\n",
    "    \"\"\"\n",
    "    데이터 표준화 (평균 0, 표준편차 1).\n",
    "    Args:\n",
    "        data (np.array): 원본 데이터, Shape: (samples, frames, joints, coords).\n",
    "    Returns:\n",
    "        np.array: 표준화된 데이터.\n",
    "    \"\"\"\n",
    "    mean = data.mean(axis=(1, 2, 3), keepdims=True)  # 샘플별 평균 계산\n",
    "    std = data.std(axis=(1, 2, 3), keepdims=True)  # 샘플별 표준편차 계산\n",
    "    standardized_data = (data - mean) / (std + 1e-8)  # 표준화\n",
    "    return standardized_data\n",
    "\n",
    "# 데이터 표준화 적용\n",
    "X_standardized = standardize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (105, 90, 33, 3), Validation shape: (22, 90, 33, 3), Test shape: (23, 90, 33, 3)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할 (훈련/검증/테스트 세트)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_standardized, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTM 출력\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 타임스텝의 출력만 사용\n",
    "        return out\n",
    "    \n",
    "# 모델 초기화\n",
    "input_dim = X_train.shape[2] * X_train.shape[3]  # 관절 수 * 좌표 수\n",
    "hidden_dim = 256\n",
    "output_dim = len(labels_map)  # 클래스 수 (등, 스쿼트, 옆구리)\n",
    "model = LSTMModel(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_dataset = TensorDataset(torch.tensor(X_train).float(), torch.tensor(y_train).long())\n",
    "val_dataset = TensorDataset(torch.tensor(X_val).float(), torch.tensor(y_val).long())\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 손실 함수 가중치 조정\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 1.0150, Validation Loss: 2.1706\n",
      "Epoch 2/100, Train Loss: 1.2582, Validation Loss: 1.0986\n",
      "Epoch 3/100, Train Loss: 1.1008, Validation Loss: 1.0964\n",
      "Epoch 4/100, Train Loss: 1.1011, Validation Loss: 1.0999\n",
      "Epoch 5/100, Train Loss: 1.1090, Validation Loss: 1.1007\n",
      "Epoch 6/100, Train Loss: 1.0995, Validation Loss: 1.0987\n",
      "Epoch 7/100, Train Loss: 1.1001, Validation Loss: 1.0977\n",
      "Epoch 8/100, Train Loss: 1.0987, Validation Loss: 1.0992\n",
      "Epoch 9/100, Train Loss: 1.1011, Validation Loss: 1.0985\n",
      "Epoch 10/100, Train Loss: 1.1020, Validation Loss: 1.0917\n",
      "Epoch 11/100, Train Loss: 1.0942, Validation Loss: 1.0919\n",
      "Epoch 12/100, Train Loss: 1.0897, Validation Loss: 1.0875\n",
      "Epoch 13/100, Train Loss: 1.0829, Validation Loss: 1.0801\n",
      "Epoch 14/100, Train Loss: 1.0769, Validation Loss: 1.0694\n",
      "Epoch 15/100, Train Loss: 1.0571, Validation Loss: 1.0440\n",
      "Epoch 16/100, Train Loss: 1.0330, Validation Loss: 1.0177\n",
      "Epoch 17/100, Train Loss: 0.9914, Validation Loss: 0.9735\n",
      "Epoch 18/100, Train Loss: 0.9409, Validation Loss: 0.8964\n",
      "Epoch 19/100, Train Loss: 0.8443, Validation Loss: 0.7747\n",
      "Epoch 20/100, Train Loss: 0.7212, Validation Loss: 0.6405\n",
      "Epoch 21/100, Train Loss: 0.5982, Validation Loss: 0.5633\n",
      "Epoch 22/100, Train Loss: 0.5316, Validation Loss: 0.4843\n",
      "Epoch 23/100, Train Loss: 0.4607, Validation Loss: 0.4148\n",
      "Epoch 24/100, Train Loss: 0.3722, Validation Loss: 0.3475\n",
      "Epoch 25/100, Train Loss: 0.3059, Validation Loss: 0.2783\n",
      "Epoch 26/100, Train Loss: 0.2466, Validation Loss: 0.2453\n",
      "Epoch 27/100, Train Loss: 0.2172, Validation Loss: 0.2146\n",
      "Epoch 28/100, Train Loss: 0.1927, Validation Loss: 0.1898\n",
      "Epoch 29/100, Train Loss: 0.1689, Validation Loss: 0.1663\n",
      "Epoch 30/100, Train Loss: 0.1482, Validation Loss: 0.1472\n",
      "Epoch 31/100, Train Loss: 0.1387, Validation Loss: 0.1393\n",
      "Epoch 32/100, Train Loss: 0.1274, Validation Loss: 0.1320\n",
      "Epoch 33/100, Train Loss: 0.1191, Validation Loss: 0.1257\n",
      "Epoch 34/100, Train Loss: 0.1134, Validation Loss: 0.1197\n",
      "Epoch 35/100, Train Loss: 0.1091, Validation Loss: 0.1145\n",
      "Epoch 36/100, Train Loss: 0.1053, Validation Loss: 0.1119\n",
      "Epoch 37/100, Train Loss: 0.1068, Validation Loss: 0.1095\n",
      "Epoch 38/100, Train Loss: 0.1044, Validation Loss: 0.1070\n",
      "Epoch 39/100, Train Loss: 0.1021, Validation Loss: 0.1047\n",
      "Epoch 40/100, Train Loss: 0.0999, Validation Loss: 0.1025\n",
      "Epoch 41/100, Train Loss: 0.0935, Validation Loss: 0.1014\n",
      "Epoch 42/100, Train Loss: 0.0925, Validation Loss: 0.1004\n",
      "Epoch 43/100, Train Loss: 0.0925, Validation Loss: 0.0994\n",
      "Epoch 44/100, Train Loss: 0.0952, Validation Loss: 0.0984\n",
      "Epoch 45/100, Train Loss: 0.0943, Validation Loss: 0.0974\n",
      "Epoch 46/100, Train Loss: 0.0888, Validation Loss: 0.0969\n",
      "Epoch 47/100, Train Loss: 0.0930, Validation Loss: 0.0964\n",
      "Epoch 48/100, Train Loss: 0.0925, Validation Loss: 0.0959\n",
      "Epoch 49/100, Train Loss: 0.0874, Validation Loss: 0.0954\n",
      "Epoch 50/100, Train Loss: 0.0916, Validation Loss: 0.0949\n",
      "Epoch 51/100, Train Loss: 0.0872, Validation Loss: 0.0947\n",
      "Epoch 52/100, Train Loss: 0.0872, Validation Loss: 0.0944\n",
      "Epoch 53/100, Train Loss: 0.0907, Validation Loss: 0.0942\n",
      "Epoch 54/100, Train Loss: 0.0905, Validation Loss: 0.0939\n",
      "Epoch 55/100, Train Loss: 0.0857, Validation Loss: 0.0937\n",
      "Epoch 56/100, Train Loss: 0.0901, Validation Loss: 0.0936\n",
      "Epoch 57/100, Train Loss: 0.0900, Validation Loss: 0.0934\n",
      "Epoch 58/100, Train Loss: 0.0853, Validation Loss: 0.0933\n",
      "Epoch 59/100, Train Loss: 0.0857, Validation Loss: 0.0932\n",
      "Epoch 60/100, Train Loss: 0.0851, Validation Loss: 0.0931\n",
      "Epoch 61/100, Train Loss: 0.0895, Validation Loss: 0.0930\n",
      "Epoch 62/100, Train Loss: 0.0857, Validation Loss: 0.0929\n",
      "Epoch 63/100, Train Loss: 0.0894, Validation Loss: 0.0929\n",
      "Epoch 64/100, Train Loss: 0.0857, Validation Loss: 0.0928\n",
      "Epoch 65/100, Train Loss: 0.0855, Validation Loss: 0.0928\n",
      "Epoch 66/100, Train Loss: 0.0855, Validation Loss: 0.0927\n",
      "Epoch 67/100, Train Loss: 0.0892, Validation Loss: 0.0927\n",
      "Epoch 68/100, Train Loss: 0.0846, Validation Loss: 0.0927\n",
      "Epoch 69/100, Train Loss: 0.0854, Validation Loss: 0.0926\n",
      "Epoch 70/100, Train Loss: 0.0854, Validation Loss: 0.0926\n",
      "Epoch 71/100, Train Loss: 0.0853, Validation Loss: 0.0926\n",
      "Epoch 72/100, Train Loss: 0.0851, Validation Loss: 0.0926\n",
      "Epoch 73/100, Train Loss: 0.0845, Validation Loss: 0.0925\n",
      "Epoch 74/100, Train Loss: 0.0890, Validation Loss: 0.0925\n",
      "Epoch 75/100, Train Loss: 0.0845, Validation Loss: 0.0925\n",
      "Epoch 76/100, Train Loss: 0.0853, Validation Loss: 0.0925\n",
      "Epoch 77/100, Train Loss: 0.0845, Validation Loss: 0.0925\n",
      "Epoch 78/100, Train Loss: 0.0889, Validation Loss: 0.0925\n",
      "Early stopping triggered. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# EarlyStopping 클래스 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# 학습률 스케줄러 추가\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# EarlyStopping 객체 생성\n",
    "early_stopping = EarlyStopping(patience=7, min_delta=0.0001)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 100  # 에포크를 100번으로 설정\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1, input_dim)\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # 학습률 스케줄러 업데이트\n",
    "    scheduler.step()\n",
    "\n",
    "    # 검증 단계\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.view(X_batch.size(0), -1, input_dim)\n",
    "            y_pred = model(X_batch)\n",
    "            val_loss += criterion(y_pred, y_batch).item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "    # Early Stopping 확인\n",
    "    early_stopping(val_loss / len(val_loader))\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered. Training stopped.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data min: -0.8864067792892456, Data max: 1.0206526517868042\n",
      "Class distribution: [50 50 50]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data min: {X.min()}, Data max: {X.max()}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        back       1.00      1.00      1.00         7\n",
      "       squat       1.00      1.00      1.00         8\n",
      "        side       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        23\n",
      "   macro avg       1.00      1.00      1.00        23\n",
      "weighted avg       1.00      1.00      1.00        23\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 평가\n",
    "from sklearn.metrics import classification_report\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "test_loader = DataLoader(TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).long()), batch_size=4)\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1, input_dim)  # 차원 변환\n",
    "        outputs = model(X_batch)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(y_batch.numpy())\n",
    "        y_pred.extend(preds.numpy())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=list(labels_map.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 0 0]\n",
      " [0 8 0]\n",
      " [0 0 8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scripted model saved to C:/Users/Admin/Desktop\\lstm_model_scripted.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "'''\n",
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # LSTM 출력\n",
    "        out = self.fc(out[:, -1, :])  # 마지막 타임스텝의 출력만 사용\n",
    "        return out\n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = 34  # 입력 크기: 34\n",
    "hidden_dim = 64  # LSTM의 은닉 상태 크기\n",
    "output_dim = 3  # 출력 클래스 수 (e.g., Back, Squat, Side)\n",
    "num_layers = 3  # LSTM 레이어 수\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim, num_layers)\n",
    "'''\n",
    "\n",
    "# TorchScript로 변환\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# 경로 설정\n",
    "output_dir = \"C:/Users/Admin/Desktop\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # 디렉토리가 없으면 생성\n",
    "model_path = os.path.join(output_dir, \"lstm_model_scripted.pt\")\n",
    "\n",
    "# TorchScript 모델 저장\n",
    "scripted_model.save(model_path)\n",
    "print(f\"Scripted model saved to {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
